name: Python application with Integration Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    services:
      ollama:
        image: ollama/ollama
        # We need to mount a volume so the cache action can access the model files
        volumes:
          - ollama_models:/root/.ollama
        ports:
          - 11434:11434

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python 3.9
        uses: actions/setup-python@v4
        with:
          python-version: 3.9

      - name: Cache Ollama models
        uses: actions/cache@v4
        with:
          # The path inside the runner that we want to cache
          path: /var/lib/docker/volumes/ollama_models/_data
          # A unique key for the cache. If the key matches, the cache is restored.
          key: ${{ runner.os }}-ollama-gemma-2b

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          sudo apt-get update && sudo apt-get install -y curl

      - name: Wait for Ollama to be ready
        run: |
          echo "Waiting for Ollama service to start..."
          timeout 60s bash -c 'until curl -s -f http://localhost:11434/api/tags > /dev/null; do echo "Still waiting..."; sleep 2; done'
          echo "Ollama service is up and running!"

      - name: Pull LLM model if not cached
        run: |
          # Check if the model already exists via API (since ollama CLI is not on host)
          if [ $(curl -s -o /dev/null -w "%{http_code}" -d '{"name": "gemma:2b"}' http://localhost:11434/api/show) -ne 200 ]; then
            echo "Model not found in cache, pulling from registry..."
            curl -N -X POST http://localhost:11434/api/pull -d '{"name": "gemma:2b"}'
          else
            echo "Model 'gemma:2b' restored from cache."
          fi

      - name: Run Unit Tests
        run: python -m pytest -m unit

      - name: Run Integration Tests
        env:
          RUN_INTEGRATION_TESTS: "1"
          LLM_ENDPOINT: "http://localhost:11434/v1"
          LLM_MODEL_NAME: "gemma:2b"
        run: |
          mkdir -p data
          python -m pytest -m integration

      - name: Fix cache permissions
        if: always()
        run: sudo chown -R $USER /var/lib/docker/volumes/ollama_models/_data